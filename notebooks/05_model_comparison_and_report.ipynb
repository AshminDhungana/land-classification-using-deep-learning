{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6214dd76",
   "metadata": {},
   "source": [
    "#  Comparative Analysis of Keras and PyTorch Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3940d5",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f65131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#General imports\n",
    "import os \n",
    "import time \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#sklearn imports\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import HeUniform\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "gpu_list = tf.config.list_physical_devices('GPU')\n",
    "device = \"gpu\" if gpu_list != [] else \"cpu\"\n",
    "print(f\"Device available for training: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff11d9d",
   "metadata": {},
   "source": [
    "### 2. Evaluation metrics \n",
    "\n",
    "The following metrics are used for evaluation of various AI/ML models:\n",
    "    \n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "- Confusion matrix\n",
    "- Receiver Operating Characteristic - Area Under Curve (ROC-AUC)\n",
    "\n",
    "**1. Accuracy**\n",
    "\n",
    "**Definition:**\n",
    "Accuracy is the proportion of correct predictions (both true positives and true negatives) among the total number of cases examined. In other words, it measures how often the classifier is correct overall.\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "Accuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\\]\n",
    "\n",
    "- TP: True positives (correctly predicted positive cases)\n",
    "- TN: True negatives (correctly predicted negative cases)\n",
    "- FP: False positives (incorrectly predicted positive cases)\n",
    "- FN: False negatives (incorrectly predicted negative cases)\n",
    "\n",
    "**Significance:**\n",
    "\n",
    "Accuracy is intuitive and easy to interpret, making it a common first metric for model evaluation. However, it can be misleading if the dataset is imbalanced (i.e., one class is much more frequent than the other). This is because a model can achieve high accuracy by simply predicting the majority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc3b06e",
   "metadata": {},
   "source": [
    "**2. Precision**\n",
    "\n",
    "**Definition:**\n",
    "Precision measures the proportion of positive predictions that are actually correct. It answers the question: \"Of all the samples that the model predicted as positive, how many were truly positive?\"\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "Precision = $\\frac{TP}{TP + FP}$\n",
    "\\]\n",
    "\n",
    "**Significance:**\n",
    "Precision is crucial when the cost of a false positive is high. For example, in medical diagnosis, predicting a disease when it's not present (false positive) can lead to unnecessary treatments. In land classification, high precision means that when the model predicts a tile as agricultural, it is likely correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b342d",
   "metadata": {},
   "source": [
    "**3. Recall (sensitivity or true positive rate)**\n",
    "\n",
    "**Definition:**\n",
    "Recall measures the proportion of actual positive cases that were correctly identified by the model. It answers: \"Of all the true positive samples, how many did the model identify?\"\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "Recall = $\\frac{TP}{TP + FN}$\n",
    "\\]\n",
    "\n",
    "**Significance:**\n",
    "Recall is important when the cost of missing a positive case (false negative) is high. In land classification, high recall means the model is good at finding all the agricultural land, even if it sometimes mislabels non-agricultural land as agricultural.\n",
    "\n",
    "\n",
    "**4. F1 score**\n",
    "\n",
    "**Definition:**\n",
    "The F1 score is the harmonic mean of precision and recall. It provides a single metric that balances both concerns. It is especially useful when you need to find an equilibrium between precision and recall.\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "F1 = $2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n",
    "\\]\n",
    "\n",
    "**Significance:**\n",
    "The F1 score is especially valuable when the class distribution is uneven or when both false positives and false negatives are important. It penalizes extreme values, so a model with high precision but low recall (or vice versa) will have a lower F1 score.\n",
    "\n",
    "\n",
    "**5. Confusion matrix**\n",
    "\n",
    "**Definition:**\n",
    "A confusion matrix is a table that summarizes the performance of a classification algorithm. It displays the counts of true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "|               | Predicted positive | Predicted negative |\n",
    "|---------------|-------------------|-------------------|\n",
    "| Actual positive | True positive (TP) | False negative (FN) |\n",
    "| Actual negative | False positive (FP) | True negative (TN) |\n",
    "\n",
    "**Significance:**\n",
    "The confusion matrix provides a detailed breakdown of model errors and successes, helping you understand not just how often the model is right, but *how* it is wrong. This is crucial for diagnosing issues like class imbalance or systematic misclassification.\n",
    "\n",
    "\n",
    "**6. ROC-AUC (Receiver operating characteristic - Area under curve)**\n",
    "\n",
    "**Definition:**\n",
    "ROC-AUC measures the model's ability to distinguish between classes across all possible classification thresholds. The ROC curve plots the true positive rate (recall) against the false positive rate at various thresholds. The AUC (area under the curve) summarizes this performance in a single value between 0 and 1.\n",
    "\n",
    "**Significance:**\n",
    "A model with an ROC-AUC of 1.0 perfectly distinguishes between classes, while a value of 0.5 suggests random guessing. ROC-AUC is especially useful for imbalanced datasets and when you care about the ranking of predictions rather than their absolute values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd6fbe",
   "metadata": {},
   "source": [
    "### 2.1 Function for evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e200744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             precision_score,\n",
    "                             recall_score,\n",
    "                             f1_score,\n",
    "                             roc_curve, \n",
    "                             roc_auc_score,\n",
    "                             log_loss,\n",
    "                             classification_report,\n",
    "                             confusion_matrix,\n",
    "                             ConfusionMatrixDisplay,\n",
    "                            )\n",
    "\n",
    "\n",
    "# define a function to get the metrics comprehensively\n",
    "def model_metrics(y_true, y_pred, y_prob, class_labels):\n",
    "    metrics = {'Accuracy': accuracy_score(y_true, y_pred),\n",
    "               'Precision': precision_score(y_true, y_pred),\n",
    "               'Recall': recall_score(y_true, y_pred),\n",
    "               'Loss': log_loss(y_true, y_prob),\n",
    "               'F1 Score': f1_score(y_true, y_pred),\n",
    "               'ROC-AUC': roc_auc_score(y_true, y_prob),\n",
    "               'Confusion Matrix': confusion_matrix(y_true, y_pred),\n",
    "               'Classification Report': classification_report(y_true, y_pred, target_names=class_labels, digits=4),\n",
    "               \"Class labels\": class_labels\n",
    "              }\n",
    "    return metrics\n",
    "\n",
    "#function to print the metrics\n",
    "def print_metrics(y_true, y_pred, y_prob, class_labels, model_name):\n",
    "    metrics = model_metrics(y_true, y_pred, y_prob, class_labels)\n",
    "    print(f\"Evaluation metrics for the \\033[1m{model_name}\\033[0m\")\n",
    "    print(f\"Accuracy: {'':<1}{metrics[\"Accuracy\"]:.4f}\")\n",
    "    print(f\"ROC-AUC: {'':<2}{metrics[\"ROC-AUC\"]:.4f}\")\n",
    "    print(f\"Loss: {'':<5}{metrics[\"Loss\"]:.4f}\\n\")\n",
    "    print(f\"Classification report:\\n\\n  {metrics[\"Classification Report\"]}\")\n",
    "    print(\"========= Confusion Matrix =========\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=metrics[\"Confusion Matrix\"],\n",
    "                                  display_labels=metrics[\"Class labels\"])\n",
    "\n",
    "    disp.plot()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d7d7b7",
   "metadata": {},
   "source": [
    "### 3. Model and Dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"..\", \"models\")\n",
    "\n",
    "keras_model_name = \"cnn_model_keras_best.model.keras\"\n",
    "keras_model_path = os.path.join(data_dir, keras_model_name)\n",
    "\n",
    "pytorch_state_dict_name = \"cnn_pytorch_state_dict.pth\"\n",
    "pytorch_state_dict_path = os.path.join(data_dir, pytorch_state_dict_name)\n",
    "\n",
    "\n",
    "\n",
    "dataset_path = os.path.join(os.getcwd(), \"..\", \"data\", \"images_dataSAT\")\n",
    "print(dataset_path)\n",
    "\n",
    "img_w, img_h = 64, 64\n",
    "n_channels = 3\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "\n",
    "agri_class_labels = [\"non-agri\", \"agri\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccac60e",
   "metadata": {},
   "source": [
    "### 4. Keras model evaluation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a63d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "prediction_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_w, img_h),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "keras_model = tf.keras.models.load_model(keras_model_path)\n",
    "\n",
    "steps = int(np.ceil(prediction_generator.samples / prediction_generator.batch_size))\n",
    "batch_size = int(prediction_generator.batch_size)\n",
    "print(f\"Number of Steps: {steps} with batch size: {batch_size}\")\n",
    "\n",
    "all_preds_keras = []\n",
    "all_probs_keras = []\n",
    "all_labels_keras = []\n",
    "\n",
    "for step_idx, step in enumerate(tqdm(range(steps), desc=\"Steps\")):\n",
    "    images, labels = next(prediction_generator)\n",
    "    preds = keras_model.predict(images, verbose='0')\n",
    "    all_probs_keras.extend(preds)\n",
    "    preds = (preds > 0.5).astype(int).flatten()\n",
    "    all_preds_keras.extend(preds)\n",
    "    all_labels_keras.extend(labels)\n",
    "\n",
    "print_metrics(y_true = all_labels_keras,\n",
    "              y_pred = all_preds_keras,\n",
    "              y_prob = all_probs_keras,\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"Keras Model\"\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e99583",
   "metadata": {},
   "source": [
    "### 5. PyTorch model evaluation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c38d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Processing inference on {device}\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_w, img_h)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "full_dataset = datasets.ImageFolder(dataset_path, transform=train_transform)\n",
    "test_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, 5, padding=2), nn.ReLU(),\n",
    "    nn.MaxPool2d(2), nn.BatchNorm2d(32),\n",
    "    nn.Conv2d(32, 64, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(64),\n",
    "    nn.Conv2d(64, 128, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(128),\n",
    "    nn.Conv2d(128, 256, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n",
    "    nn.Conv2d(256, 512, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n",
    "    nn.Conv2d(512, 1024, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n",
    "    nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "    nn.Linear(1024, 2048), nn.ReLU(), nn.BatchNorm1d(2048), nn.Dropout(0.4),\n",
    "    nn.Linear(2048, num_classes)\n",
    ").to(device)\n",
    "\n",
    "print(\"Created model, now loading the weights from saved model state dict\")\n",
    "model.load_state_dict(torch.load(pytorch_state_dict_path))\n",
    "print(\"Loaded model state dict, now getting predictions\")\n",
    "\n",
    "all_preds_pytorch = []\n",
    "all_labels_pytorch = []\n",
    "all_probs_pytorch = []\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(test_loader, desc=\"Step\")):\n",
    "#    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        probs = F.softmax(outputs, dim=1)[:, 1]  # probability for class 1\n",
    "        all_probs_pytorch.extend(probs.cpu())\n",
    "        all_preds_pytorch.extend(preds.cpu().numpy().flatten())\n",
    "        all_labels_pytorch.extend(labels.numpy())\n",
    "\n",
    "print_metrics(y_true = all_labels_pytorch,\n",
    "              y_pred = all_preds_pytorch,\n",
    "              y_prob = all_probs_pytorch,\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"PyTorch Model\"\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5597a927",
   "metadata": {},
   "source": [
    "### 6. ROC curve plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9dadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_roc(y_true, y_prob, model_name):\n",
    "    n_classes = y_prob.shape[1] if y_prob.ndim > 1 else 1\n",
    "    if n_classes == 1:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')\n",
    "    else:\n",
    "        y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "        for i in range(n_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "            auc = roc_auc_score(y_true_bin[:, i], y_prob[:, i])\n",
    "            plt.plot(fpr, tpr, label=f'{model_name} class {i} (AUC = {auc:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "plot_roc(np.array(all_labels_keras), np.array(all_probs_keras), \"Keras Model\")\n",
    "plt.show()\n",
    "plot_roc(np.array(all_labels_pytorch), np.array(all_probs_pytorch), \"PyTorch Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98bff7c",
   "metadata": {},
   "source": [
    "###  Comparing model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdaa721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Keras model performance metrics\n",
    "metrics_keras = model_metrics(all_labels_keras, all_preds_keras, all_probs_keras, agri_class_labels)\n",
    "\n",
    "# get the PyTorch model performance metrics\n",
    "metrics_pytorch = model_metrics(all_labels_pytorch, all_preds_pytorch, all_probs_pytorch, agri_class_labels)\n",
    "\n",
    "\n",
    "# Display the comparison of metrics\n",
    "print(\"{:<18} {:<15} {:<15}\".format('\\033[1m'+ 'Metric' + '\\033[0m',\n",
    "                                    'Keras Model', \n",
    "                                    'PyTorch Model'))\n",
    "\n",
    "mertics_list = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "\n",
    "for k in mertics_list:\n",
    "    print(\"{:<18} {:<15.4f} {:<15.4f}\".format('\\033[1m'+k+'\\033[0m',\n",
    "                                              metrics_keras[k],\n",
    "                                              metrics_pytorch[k]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
